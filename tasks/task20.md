# Task: Use LangChain with Ollama and Runnable Sequences to execute prompts in parallel

### Goal

Demonstrate how run prompts in parallel.

### Steps

1. **Initialize project**

   - Create a new folder, run `npm init -y`, and set `"type": "module"` in `package.json`.
   - Install required packages:
     - `@langchain/ollama`
     - `@langchain/core`

2. **Create helper function**

   - Instantiate a `ChatOllama` model (`llama3.2:1b` by default).
   - Create a `StringOutputParser` to extract plain text from the model's output.
   - Build a `ChatPromptTemplate` with:
     - A system message describing the task (`"Explain animals in one sentence."`)
     - A user message placeholder (`"{question}"`)
   - Combine the prompt template, model, and parser into a `RunnableSequence`.

3. **Send prompts**

   - Prepare a list of animals, e.g., `['cow', 'chicken', 'cat']`.
   - Map over the list and invoke the runnable sequence for each animal.
   - Collect all results into an array.

4. **Create demo script**

   - Print the results array to the console.
   - Each entry should be a one-sentence explanation generated by the model.

5. **Run and verify**

   - Execute the script with Node.
   - Confirm that the output contains concise explanations for each animal.
